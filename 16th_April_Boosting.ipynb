{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a870c6a-42cb-42f8-83ea-23686ba2c8e5",
   "metadata": {},
   "source": [
    "Q1. What is boosting in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "158b4c3b-ddd4-4380-9eb0-93153d0ef2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boosting is one of the Ensemble Learning Technique used to train model in a sequential way.\n",
    "## The output of multiple models also called weak models are combined to get the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd47fe1-7979-481d-ad8d-1276215a9ba1",
   "metadata": {},
   "source": [
    "Q2. What are the advantages and limitations of using boosting techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb16ddaa-d10a-4ed8-b109-dc539d295fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Advantages\n",
    "## Since multiple models are used to train it helps to avoid overfitting\n",
    "## It works in sequential manner so performance and accuracy is high since it learns from previous weak models\n",
    "## It can handle complex relationship, It is capable of modeling nonlinear interactions, handling missing values, and managing high-dimensional feature spaces effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a3eb0de-e656-4e25-b601-754f73b5f0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Limitations\n",
    "## Since It works in Sequential Manner , it can take higher time to train teh models if data is large\n",
    "## Sensitive to Noise and Outliers: Boosting algorithms can be sensitive to noisy data and outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b956f596-670e-4144-a31c-2e9bc63f0e90",
   "metadata": {},
   "source": [
    "Q3. Explain how boosting works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a8d1c1b-6c69-4862-a512-66abea31d639",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In the first step each sample is assigned equal weights so they have equal probability of selecting in training\n",
    "## Weak learner is ttained using this sample data.\n",
    "## Model performance is evaluated on the whole training data and misclassified samples are identified\n",
    "## Weights are updated and misclassified data is assined higher weights\n",
    "## The process is repeated for a predetermined number of iterations. In each iteration, the weights are updated, and a new weak learner is trained on the reweighted training data. \n",
    "##  The predictions of all weak learners are combined to create the final prediction.\n",
    "## The final model is the aggregation of all weak learners, with their weighted contributions. It can be used to make predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c7bc83-3d15-463e-b08e-6879820812f4",
   "metadata": {},
   "source": [
    "Q4. What are the different types of boosting algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0788f885-436d-483d-a0c3-16b0558642da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Different Types of Boosting Algorithm\n",
    "## 1.Adaboosting\n",
    "## 2.Gradient Boosting\n",
    "## 3.XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0125284f-8e6c-4380-add4-8e1bb50a2dcc",
   "metadata": {},
   "source": [
    "Q5. What are some common parameters in boosting algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7fe735e-06c4-4974-9a8c-389385882191",
   "metadata": {},
   "outputs": [],
   "source": [
    "## n_estimators: The maximum number of estimators at which boosting is terminated\n",
    "## learning_rate:Learning rate shrinks the contribution of each tree by learning_rate.\n",
    "## loss:The loss function to be optimized.\n",
    "## criterion:The function to measure the quality of a split. Supported criteria are ‘friedman_mse’ and ‘squared_error’\n",
    "## min_samples_split:The minimum number of samples required to split an internal node:\n",
    "## min_samples_leaf:The minimum number of samples required to be at a leaf node\n",
    "## max_depth:The maximum depth limits the number of nodes in the tree\n",
    "## max_features:The number of features to consider when looking for the best split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5c90ac-c38a-482d-9ca6-27ae75e0865c",
   "metadata": {},
   "source": [
    "Q6. How do boosting algorithms combine weak learners to create a strong learner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6d3ff92-ee92-42e7-a01e-fcf3504f67d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AS Explained in Question 3 \n",
    "## Weak models are trained and evaluated\n",
    "## The weights are updated, and wieights of misclassified data samples are given more weightage\n",
    "## The model is trained again and weights are updated, this process is repeated until we get minimum loss\n",
    "## Finally we combine the output to get the strong learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edd5506-584c-415b-bd43-11b621df9afa",
   "metadata": {},
   "source": [
    "Q7. Explain the concept of AdaBoost algorithm and its working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd0fde4d-32fb-41d2-a18a-395cd30c25b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AdaBoost (Adaptive Boosting) is a boosting algorithm that works by iteratively training a series of weak learners (base models)\n",
    "##   and adjusting the weights of training examples to focus on the misclassified examples.\n",
    "## The Working of Adaboost is same as explained in Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dda84a-8d22-4213-b3ba-0f96050c8a89",
   "metadata": {},
   "source": [
    "Q8. What is the loss function used in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11a14acb-7e2d-4b86-af99-cdf442062ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The loss function used in Adaboost is Exponential Loss Function\n",
    "## L(y, f(x)) = exp(-y * f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0708ca7-22b2-4599-a7e7-cf07188979e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here, y represents the true label of an example (either +1 or -1), f(x) represents the prediction of the weak learner \n",
    "## for that example, and L(y, f(x)) represents the loss for that example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585f69f7-8ad5-4625-8c61-15d99bbd43e3",
   "metadata": {},
   "source": [
    "Q9. How does the AdaBoost algorithm update the weights of misclassified samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1650bca0-c184-4c71-acc7-9e79641e7e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In first step we create Decison Tree Stump\n",
    "## In secomd step we calculate sum of total erros and performace of stump\n",
    "## for weights of misclassified samples we update as\n",
    "## Formula= Weight*e^(Perfromance of stump)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150eb0dc-40e7-46d0-9ee3-d3471e0485d8",
   "metadata": {},
   "source": [
    "Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9466ba3f-1dab-43fd-b96e-2340d2bafa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If Number of etimators increases ther can be postive as wel as negative impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "713836b6-a247-464a-8f89-cea2ccf65280",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Positive Impact: overall accuracy will be increased\n",
    "## More stable : With an increased number of estimators, the AdaBoost model's predictions tend to become more stable and less sensitive to individual weak learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4092a031-9ac2-4560-8599-e6a96c4f6577",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Negative Ipact:\n",
    "## It can be computationaly expensive as large number of estimators are used:\n",
    "## It will take much larger time to compute as it works sequentialy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ae3e02-9214-47a5-b0af-6a096c2716e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
