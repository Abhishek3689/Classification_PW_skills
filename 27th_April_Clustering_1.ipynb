{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "330a9778-4867-42a4-a44e-eec13b548c0f",
   "metadata": {},
   "source": [
    "Q1. What are the different types of clustering algorithms, and how do they differ in terms of their approach\n",
    "and underlying assumptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e8bb6ff-2d6f-415d-9592-757dbad144f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## There are basic 3 types of clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f437c1e-fb62-41a8-b0e9-06088dd3cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. K Means Clustering\n",
    "## 2. Heirarchical clustering\n",
    "## 3. DB Scan Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f49bbf5-ee52-4e4e-aa99-5778f125d34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## K means Clustering: In this approach initially some random centroids are selected and nearest data points with these centroids are collected in the form of cluster.\n",
    "## The centroid position is adjusted and again same procedure as above is calculated to form new cluster until convergence\n",
    "## Assumptions: Assumes clusters as spherical and of equal size, and the data points within each cluster have similar variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcffdb13-27b9-4c8f-8ff0-72167247bb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hierarchical Clustering:In this approach each data points is considered as cluster at first step.\n",
    "## then pair of nearest clusters is combined to create a new cluster.\n",
    "## In this way above steps are repeated until a hierarchical structure is formed.\n",
    "## Assumptions: Does not assume specific shapes or sizes of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9865e64b-b19e-4b4f-9492-12feea714e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DB Scan Clustering: It identifies dense regions by forming clusters around data points with sufficient nearby neighbors and separates noise points.\n",
    "## In this approach non linear data is also seperable.\n",
    "## Assumptions: Assumes clusters as dense regions separated by sparser regions and can discover clusters of arbitrary shape and size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a098d44e-6d27-47dc-a1b6-ddb0f7a9549c",
   "metadata": {},
   "source": [
    "Q2.What is K-means clustering, and how does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b25208cd-ff69-4ea9-aa61-e6896faec9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A K-means clustering algorithm tries to group similar items in the form of clusters. The number of groups is represented by K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dcaa849-7664-4be6-98ab-de57b1c13a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Steps\n",
    "## 1. Select the value of k\n",
    "## 2. Initialize the k Centroids ie. selecting random data points as centroids\n",
    "## 3. Assign the data points to nearest cluster centroids\n",
    "## 4. we will re-initialize the centroids by calculating the average of all data points of that cluster.\n",
    "## 5.We will keep repeating steps 3 and 4 until we have optimal centroids and the assignments of data points to correct clusters are not changing anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a74cdb-ae39-43e8-8de6-ddde4a62ab3a",
   "metadata": {},
   "source": [
    "Q3. What are some advantages and limitations of K-means clustering compared to other clustering\n",
    "techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c2da75c-998b-40da-a378-a6911fe6b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bed8e4a-6e8c-4a43-bd0b-e8b3ee1e1069",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple to implement.\n",
    "##  Scalable:K-means clustering is computationally efficient and can handle large datasets with a moderate number of clusters.\n",
    "## K-means clustering is suitable for partitioning data into non-hierarchical clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "babf8293-380c-4699-8406-31f17864a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5c6b78d-7fce-434f-b432-c4795964bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We need to initialize k values\n",
    "## Can not perform well for non linear data\n",
    "## Prone to ouliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabe1167-1029-4c9f-8d98-813f16341f63",
   "metadata": {},
   "source": [
    "Q4. How do you determine the optimal number of clusters in K-means clustering, and what are some\n",
    "common methods for doing so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81a084bf-ddb7-4ede-a0f2-24e36b21d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To determine the optimal value of k in K means clustering we can use\n",
    "## 1. Elbow method(ploting the graph between range of k value and Within Cluster sum of squares(WCSS) and finding the best bending point)\n",
    "## 2. KNee_locator: this will give you elbow ie the k value\n",
    "## 3. Silhoutee_score : by using this we can find the hihgest value of silhouette coefficients for range in k "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f45c5b4-b226-4739-af95-15c37d668be9",
   "metadata": {},
   "source": [
    "Q5. What are some applications of K-means clustering in real-world scenarios, and how has it been used\n",
    "to solve specific problems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a93ee6ef-aa0c-41c1-af9f-ee431efe9a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## There are multiple application where K means clustering can be used\n",
    "## 1. Image classifiy: when there are large number of unlabeled images , cluster can be used to group similar images togehter for eg cat and dog\n",
    "## 2. Customer Segment: In Banking Customer can be segregated using banking transactions and can help to identify customer for upsell/cross-sell\n",
    "## 3. Anomaly Detection: By clustering data points into normal and anomalous groups, K-means clustering can be used for detecting outliers\n",
    "##     or unusual patterns in datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73ba99d-351f-4eda-91b0-a6fb87e1d8c9",
   "metadata": {},
   "source": [
    "Q6. How do you interpret the output of a K-means clustering algorithm, and what insights can you derive\n",
    "from the resulting clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ac8f56c-2094-4fe5-94cb-20c7b82bd3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WE can use visualization (scatterplot ) to see how well the dataset has been clustered.\n",
    "## By visualising the clusters we can infer how dense are the individual cluster , we can gain insights into the structure, patterns, \n",
    "## and characteristics of your data shows some pattern.\n",
    "##  Analyzing the distribution of data points across clusters helps understand the relative importance or prevalence of different groups within the dataset.\n",
    "## The spatial separation between clusters can provide insights into the distinctness of different groups in the dataset.\n",
    "## These insights can guide decision-making, pattern recognition, segmentation, or further analysis tasks, depending on the specific objectives of your study or problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4c4d26-bd9b-4768-a977-344cd6f87e2b",
   "metadata": {},
   "source": [
    "Q7. What are some common challenges in implementing K-means clustering, and how can you address\n",
    "them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00a72243-4d4a-4e16-965a-73ed67304a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Common Challenges\n",
    "## 1. Choosing the right number of cluster k: This can be addressed by elbow method or slihouette score\n",
    "## 2. Handling Outliers: Outliers can significantly affect the centroid positions and cluster assignments in K-means clustering.\n",
    "##     One approach is to detect and remove outliers before applying the clustering algorithm\n",
    "## 3.  K-means assumes spherical clusters of similar sizes. If your data contains clusters with different shapes or overlapping regions, \n",
    "##     K-means may not perform well. In such cases, considering alternative clustering algorithms like DBSCAN can be used in usch case\n",
    "## 4. Feature Scaling: K-means clustering is influenced by the scale of features, and variables with larger scales can dominate the clustering process. \n",
    "##     It is important to normalize or standardize the features before applying K-means to ensure all variables contribute equally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdd6204-c89c-4bf3-8874-2a2df16ff6a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
