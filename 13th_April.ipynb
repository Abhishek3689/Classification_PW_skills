{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a8967ba-d6da-474b-852b-525cfd70389f",
   "metadata": {},
   "source": [
    "Q1. What is Random Forest Regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aa2df8d-7cab-4491-aa0b-311d9167b25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random forest Regressor is ensemble learning  which uses bootstraping method to select random subsets with replacement to solve \n",
    "## Regression Problem.\n",
    "## Here output of multiple base learners are combined to give final outpur which is the average of all models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b8090e-e60e-4029-9d51-bbfc340c5067",
   "metadata": {},
   "source": [
    "Q2. How does Random Forest Regressor reduce the risk of overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1976665b-d221-45b1-ac26-8a0eb0997b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Forest utilises random selection of subset of features and random selection of subset of rows \n",
    "## By using these two techniques, Random Forest Regressor creates a diverse set of decision trees that collectively work to make more accurate predictions\n",
    "## The model is less likely to overfit because it uses only a subset of features and training data for each tree, which helps to prevent the trees from learning the noise in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a3f11a-a3e7-4300-9b89-692c9398857f",
   "metadata": {},
   "source": [
    "Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d03c62e6-c6ec-4495-8bac-7dd40026637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### In random forest regressor multiple random subset of data is train by base learners which are multiple decision tree regressor\n",
    "### all these base learners provide an output \n",
    "## These output are aggregated and average of all the prediction are taken out to give the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ebb403-2428-45d9-91fc-f8a065175e40",
   "metadata": {},
   "source": [
    "Q4. What are the hyperparameters of Random Forest Regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "143d6ba3-f17e-4290-8d4e-0f422b6d5a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## There are multiple parameters for hyperparameters in RandomForestResgressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e15b5ea-b8d4-463d-a5d1-f03f2d05ade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## n_estimators:Number of tress in the forest\n",
    "## criterion:{“squared_error”, “absolute_error”, “friedman_mse”, “poisson”} ,The function to measure the quality of a split.\n",
    "## max_depth: The maximum depth of each decision tree. It controls the maximum number of levels in the decision tree.\n",
    "## min_samples_split: The minimum number of samples required to split an internal node.\n",
    "## min_samples_leaf: The minimum number of samples required to be at a leaf node.\n",
    "## max_features: The number of features to consider when looking for the best split. It can be a number, a fraction, or “sqrt” (square root of the total number of features).\n",
    "## bootstrap: Whether to use bootstrap samples when building trees.\n",
    "## oob_score:Whether to use out-of-bag samples to estimate the generalization score\n",
    "## random_state: The seed value used by the random number generator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a056f7-dd21-477d-a961-792a05b3b4bc",
   "metadata": {},
   "source": [
    "Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20bd4a78-ca92-4ca9-8089-5c4ff109ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Forest Regressor Multiple Decision Trees are used as base learners and final output is combined to give final output\n",
    "## While Decision Tree Regressor a sindle Decison tree is used to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76edffca-cf90-473d-92eb-28d6352783a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decision Tree Regressor suffers from overfiting issue\n",
    "## Random Fores Regressor overcomes the problem of overfitting because of random sampling of multiple Decison trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "285b2cd9-c9f6-4d3a-a19d-ef0560b42ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decision Tree Regressor is simpler, faster, and easier to interpret than Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d592bd7b-895a-4f10-94a9-ea7b1efc3106",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The overall performance of Random Forest is high as compared to Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e835f124-b645-415d-ad8c-071f282df1e9",
   "metadata": {},
   "source": [
    "Q6. What are the advantages and disadvantages of Random Forest Regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff708932-ec17-4866-b809-f0c560fae772",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Advantages:\n",
    "## High Accuracy:Random Forest Regressor is a powerful algorithm that can achieve high accuracy in predicting continuous numerical values\n",
    "## Robustness to Outliers: Random Forest Regressor is less sensitive to outliers than other regression algorithms.\n",
    "## Nonlinear Relationships: Random Forest Regressor can capture nonlinear relationships between the features and target variable\n",
    "## Overfitting Prevention: Random Forest Regressor can prevent overfitting by averaging the predictions of multiple decision trees.\n",
    "## Feature Importance: Random Forest Regressor can provide information on the importance of the features used in making predictions\n",
    "## Scalability: Random Forest Regressor can handle large datasets with high dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0228aa19-b309-41f1-af86-7f3def106171",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Disadvantages\n",
    "## Random Forest Regressor is a complex algorithm and can be difficult to interpret, which can be a disadvantage in some use cases.\n",
    "## Computational Resources: Random Forest Regressor can be computationally expensive, especially for large datasets with many trees.\n",
    "## Training Time: Random Forest Regressor can take longer to train than some other regression algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4c30a0-6327-4ea6-9ded-19f5b860ae88",
   "metadata": {},
   "source": [
    "Q7. What is the output of Random Forest Regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "794effbc-0781-4872-b0ad-d093c47fa0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The output of a Random Forest Regressor is a set of predicted continuous numerical values. For each input data point, the Random Forest Regressor \n",
    "## makes a prediction based on the majority vote of the predictions of all the decision trees in the forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d1f594-4ad5-4bcf-a224-001ce04cc7b5",
   "metadata": {},
   "source": [
    "Q8. Can Random Forest Regressor be used for classification tasks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48211ac4-f29e-49b9-962f-54f16954f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "## No Random Forest REgressor is especially build for Regressor task , for classification task Random Forest Classifier can be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bec17d-fa44-462d-bd8e-ba7bdf836361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
