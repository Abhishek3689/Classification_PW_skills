{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c9828c-3f40-4c46-856a-e96be941fda8",
   "metadata": {},
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance\n",
    "metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1563d3c7-96d7-4d0e-90e2-a00c218c50cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ## Euclidean Distance: This is the shortest  distance between points using a straight line\n",
    "## Formula=sqrt((x2-x1}^2+(y2-y1)^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18d9336d-db14-405a-8087-695849d99089",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manhattan Distance:Manhattan distance, also known as city block distance or L1 distance, measures the distance between two points by \n",
    "## summing the absolute differences of their corresponding feature values. \n",
    "## Distance = |x2 - x1| + |y2 - y1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5db61bc4-4a61-4eb3-83ab-2f7ccca14b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manhattan distance is preferred over Euclidean distance when we have a case of high dimensionality.\n",
    "##  Euclidean distance considers the direct path between two points and is sensitive to the scale of individual features.so Feature Scaling is impotant in Eucliean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d088fc5e-3d4b-48e6-aaa5-efec8ee4e299",
   "metadata": {},
   "source": [
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be\n",
    "used to determine the optimal k value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58d4af5f-d5d7-4902-96a5-f7ac45fa50cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To choose an optimal value of K we can use multiple techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2deddab9-e984-4e4b-927a-b311c84e9b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.GridSearch CV:\n",
    "## 2.Elbow Method: In this method, you plot the values of K on the x-axis and the corresponding error (or another performance metric) on the y-axis. \n",
    "##  The plot may resemble an arm, and the \"elbow\" point on the plot represents the value of K \n",
    "## 3. Cross validation using K fold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f806581f-d16a-401f-bf19-ecc4c839bb21",
   "metadata": {},
   "source": [
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In\n",
    "what situations might you choose one distance metric over the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94c85aff-8d51-4876-8295-9506fce04f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Euclean Distance : This is the shortest distance between data points i.e straight line\n",
    "## It works well when the features are continuous and have similar scales. However, it can be sensitive to outliers and high-dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df85bdb4-c753-4fc1-a264-a490d920704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manhattan Distance: Also known as the L1 norm or taxicab distance, Manhattan distance calculates the sum of absolute differences between the coordinates of two points. \n",
    "## It is more robust to outliers and performs better when dealing with features that are not on the same scale or when the data has a grid-like structure (e.g., images, text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebb1d385-d1fd-4bb3-b3e4-b4d285c494c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Minkowski Distance: Minkowski distance is a generalized distance metric that includes both Euclidean and Manhattan distances as special cases. \n",
    "## It is controlled by a parameter 'p,' where p=2 corresponds to Euclidean distance and p=1 corresponds to Manhattan distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee6c3eb2-a9d7-4f60-915d-d73c8f61cbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cosine Similarity: Cosine similarity measures the cosine of the angle between two vectors, rather than their absolute distances. \n",
    "## It is commonly used when dealing with text data or high-dimensional sparse data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac7710-8e34-4d58-a9e0-2540718f453e",
   "metadata": {},
   "source": [
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
    "the performance of the model? How might you go about tuning these hyperparameters to improve\n",
    "model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43c55194-b8a2-4fc5-936b-0d444eb95d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Common Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5744fbdc-ffa0-47b2-8a52-fb12a75ca98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## n_neighbors:Number of neighbors to use.\n",
    "##     A smaller K value makes the model more sensitive to noise and can lead to overfitting, while a larger K value can smooth out the decision boundaries but may lead to underfitting.\n",
    "## algorithm: ‘ball_tree’, ‘kd_tree’, ‘brute’\n",
    "##   The choice of algorithm can affect the training and prediction time of the model. For small datasets, brute force may be sufficient, while for larger datasets, \n",
    "##   tree-based algorithms can provide faster results.\n",
    "## Distance Metric: The distance metric determines how the similarity or dissimilarity between data points is measured\n",
    "## weights{‘uniform’, ‘distance’}:uniform weights. All points in each neighborhood are weighted equally.\n",
    "##      distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.\n",
    "## leaf size:Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa395ded-9aac-4900-810c-6bc2de9a431d",
   "metadata": {},
   "source": [
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What\n",
    "techniques can be used to optimize the size of the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31a97b6c-d2da-4ab4-88da-a0ae807ad204",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overfitting and Underfitting: With a small training set, KNN models may suffer from overfitting. When the training set is small, \n",
    "## the model can memorize the limited examples and fail to generalize well to unseen data. On the other hand,\n",
    "##if the training set is excessively large, the model may underfit the data, failing to capture the underlying patterns and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3715a46b-ce56-4baf-90d4-ec519623f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bias-Variance Tradeoff: The size of the training set influences the bias-variance tradeoff. With a small training set,\n",
    "## the model tends to have high variance, leading to a higher risk of overfitting. As the training set size increases, the model's variance decreases, but the bias may increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e67f5747-2c3a-4570-b0e5-4434daeb64e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To optimize the size of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42a9665b-a470-48c0-a0df-9f15d01989d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Augmentation:If small training se is tehre we can use oversampling,Smote techniques\n",
    "## Sampling Techniques: If large dataset is there we can random select small data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b6c4f1-b47e-4143-bb20-a47be23d3bf1",
   "metadata": {},
   "source": [
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
    "overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba9fbd12-5786-4b07-9a7e-ecbad3de37cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  1. Does not work well with large dataset: In large datasets, the cost of calculating the distance between the new point and each\n",
    "## existing points is huge which degrades the performance of the algorithm.\n",
    "##   To mitigate this issue, you can use techniques like approximate nearest neighbor algorithms (e.g., k-d tree, ball tree) to speed up the neighbor search process\n",
    "## 2. KNN calculates distances between data points, making it sensitive to feature scales.\n",
    "##       Need feature scaling: We need to do feature scaling (standardization and normalization) before applying KNN algorithm to any dataset\n",
    "## 3.Sensitive to noisy data, missing values and outliers: KNN is sensitive to noise in the dataset. We need to manually impute missing values and remove outliers.\n",
    "## 4. We need to calculate optimal value of K:Hyperparameter tuning and elbow methd can be used to find appropriate K.\n",
    "## 5.Curse of Dimensionality: KNN performance can degrade in high-dimensional spaces due to the \"curse of dimensionality.\n",
    "##   Dimensionality reduction techniques, such as PCA or feature selection methods, an be used to  overcome this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ef36a0-5e75-46f9-b955-00978f190c33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
